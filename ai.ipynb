{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 받고 텍스트로 변환해서 이미지 생성하기\n",
    "\n",
    "from flask import Flask, request, send_file\n",
    "from werkzeug.utils import secure_filename\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "def setup_model():\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "    feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    return model, feature_extractor, tokenizer, device\n",
    "\n",
    "def predict_step(image_paths, model, feature_extractor, tokenizer, device):\n",
    "    max_length = 16\n",
    "    num_beams = 4\n",
    "    gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
    "\n",
    "    images = []\n",
    "    for image_path in image_paths:\n",
    "        i_image = Image.open(image_path)\n",
    "        if i_image.mode != \"RGB\":\n",
    "            i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "        images.append(i_image)\n",
    "\n",
    "    pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "\n",
    "    output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    \n",
    "    return preds\n",
    " \n",
    "@app.route('/submit', methods=['POST'])\n",
    "def submit():\n",
    "\n",
    "    f = request.files['file']\n",
    "    file_path = 'D:/stable/' + secure_filename(f.filename)\n",
    "    f.save(file_path)\n",
    "\n",
    "    model, feature_extractor, tokenizer, device = setup_model()\n",
    "    text=predict_step([file_path], model, feature_extractor, tokenizer, device)\n",
    "    print(text)\n",
    "    \n",
    "    model_id = \"artificialguybr/freedom\"\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "    pipe = pipe.to(\"cuda\")\n",
    "\n",
    "    prompt = \"cute cartoon sticker \" + ','.join(text)\n",
    "    image = pipe(prompt).images[0]    \n",
    "    image.save(\"D:/stable/out/0001.png\")\n",
    "    print(prompt)\n",
    "\n",
    "    model_id=0\n",
    "    pipe=0\n",
    "\n",
    "    model_id2 = \"runwayml/stable-diffusion-v1-5\"\n",
    "    pipe2 = StableDiffusionPipeline.from_pretrained(model_id2, torch_dtype=torch.float16)\n",
    "    pipe2 = pipe2.to(\"cuda\")\n",
    "\n",
    "    image = pipe2(prompt).images[0]    \n",
    "    image.save(\"D:/stable/out/0002.png\")\n",
    "\n",
    "    model_id2=0\n",
    "    pipe2=0\n",
    "\n",
    "    model_id3 = \"stablediffusionapi/anything-v5\"\n",
    "    pipe3 = StableDiffusionPipeline.from_pretrained(model_id3, torch_dtype=torch.float16)\n",
    "    pipe3 = pipe3.to(\"cuda\")\n",
    "       \n",
    "    image = pipe3(prompt).images[0]    \n",
    "    image.save(\"D:/stable/out/0003.png\")\n",
    "\n",
    "    file_paths = ['D:/stable/out/0001.png', 'D:/stable/out/0002.png', 'D:/stable/out/0003.png']\n",
    "    files = [('files', (open(file, 'rb'))) for file in file_paths]\n",
    "\n",
    "    # POST 요청을 보냅니다.\n",
    "    res = requests.post('http://192.168.0.65:8080/test/multipart3', files=files)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
